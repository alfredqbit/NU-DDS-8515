{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQL0YAznNGuCJNBYpVlmyj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfredqbit/NU-DDS-8515/blob/main/sepulvedaADDS-8515-8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # K-Means Clustering and Discriminant Analysis on the Mall Customers Dataset\n",
        "\n",
        "This notebook:\n",
        " - Loads the Mall Customers dataset from Kaggle (or a public mirror).\n",
        " - Performs K-Means clustering on standardized annual income and spending score.\n",
        " - Uses the Elbow method and silhouette analysis to select the number of clusters.\n",
        " - Visualizes the resulting clusters and centroids.\n",
        " - Uses the K-Means clusters as labels for LDA and QDA classification.\n",
        " - Evaluates and visualizes LDA and QDA performance and decision boundaries.\n",
        "\n",
        "All plots are saved into a `figures/` subdirectory for inclusion in the LaTeX report."
      ],
      "metadata": {
        "id": "tL1CkX1K3KZ0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BgozeHv03Dic"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    accuracy_score,\n",
        ")\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "FIG_DIR = Path(\"figures\")\n",
        "FIG_DIR.mkdir(exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load and inspect the Mall Customers dataset\n",
        "#\n",
        "The dataset is originally from Kaggle:\n",
        "https://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python\n",
        "#\n",
        "We try, in order:\n",
        " 1. Kaggle input path (when running on Kaggle).\n",
        " 2. Local CSV (e.g., after manual download).\n",
        " 3. A public GitHub mirror of the same CSV."
      ],
      "metadata": {
        "id": "NFA3tpC33iFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_mall_customers(\n",
        "    source: str = \"auto\",\n",
        "    local_path: str = \"Mall_Customers.csv\",\n",
        ") -> pd.DataFrame:\n",
        "    # Load the Mall Customers dataset.\n",
        "\n",
        "    # Parameters\n",
        "    # ----------\n",
        "    #source : {'auto', 'local', 'url'}\n",
        "    #    - 'auto': try Kaggle path, then local_path, then public URL.\n",
        "    #    - 'local': require Mall_Customers.csv at local_path.\n",
        "    #    - 'url': use a GitHub raw mirror of the Kaggle CSV.\n",
        "    # local_path : str\n",
        "    #    Path to a local Mall_Customers.csv file.\n",
        "\n",
        "    #Returns\n",
        "    # -------\n",
        "    # df : pandas.DataFrame\n",
        "\n",
        "    # 1. Kaggle standard path\n",
        "    kaggle_path = Path(\n",
        "        \"/kaggle/input/customer-segmentation-tutorial-in-python/Mall_Customers.csv\"\n",
        "    )\n",
        "    if source == \"auto\" and kaggle_path.exists():\n",
        "        print(f\"Loading dataset from Kaggle path: {kaggle_path}\")\n",
        "        return pd.read_csv(kaggle_path)\n",
        "\n",
        "    # 2. Local CSV\n",
        "    local_file = Path(local_path)\n",
        "    if source in (\"auto\", \"local\") and local_file.exists():\n",
        "        print(f\"Loading dataset from local file: {local_file}\")\n",
        "        return pd.read_csv(local_file)\n",
        "\n",
        "    # 3. Public GitHub mirror (raw CSV)\n",
        "    if source in (\"auto\", \"url\"):\n",
        "        url = \"https://raw.githubusercontent.com/e-vdb/Mall-customers-clustering/main/Mall_Customers.csv\"\n",
        "        print(f\"Loading dataset from GitHub raw URL: {url}\")\n",
        "        return pd.read_csv(url)\n",
        "\n",
        "    raise FileNotFoundError(\"Could not load Mall_Customers.csv from any source.\")\n",
        "\n",
        "\n",
        "df = load_mall_customers(source=\"auto\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "gnxrmLmD3skt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic structure and summary statistics.\n"
      ],
      "metadata": {
        "id": "UqBV2dgD3wcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n",
        "df.describe(include=\"all\")"
      ],
      "metadata": {
        "id": "8ondBWLb30YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Feature selection and scaling\n",
        "#\n",
        "We use the two numerical behavioral features most directly related to purchasing behavior:\n",
        " - `Annual Income (k$)`\n",
        " - `Spending Score (1-100)`\n",
        "#\n",
        " CustomerID is an identifier and is dropped. Gender and Age are left out of the clustering\n",
        " so we can work in a 2D space that is easy to visualize and interpret."
      ],
      "metadata": {
        "id": "dnJ4PCxV35jB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_cols = [\"Annual Income (k$)\", \"Spending Score (1-100)\"]\n",
        "X = df[feature_cols].copy()\n",
        "\n",
        "# Quick scatter plot to see structure\n",
        "plt.figure()\n",
        "sns.scatterplot(\n",
        "    data=df,\n",
        "    x=\"Annual Income (k$)\",\n",
        "    y=\"Spending Score (1-100)\"\n",
        ")\n",
        "plt.title(\"Raw data: Annual Income vs Spending Score\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"raw_income_spend_scatter.png\", dpi=300)\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "sdK3Z3764Bc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. K-Means clustering: Elbow method and silhouette analysis\n",
        "#\n",
        " We standardize the features and run K-Means for K = 2,...,10.\n",
        " For each K we record:\n",
        " - inertia (within-cluster sum of squares)\n",
        " - average silhouette score\n",
        "#\n",
        "Then we choose K guided by these diagnostics (often K≈5 for this dataset)."
      ],
      "metadata": {
        "id": "m5LP_9w84EbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "K_range = range(2, 11)\n",
        "inertias = []\n",
        "sil_scores = []\n",
        "\n",
        "for K in K_range:\n",
        "    pipe = Pipeline(\n",
        "        steps=[\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"kmeans\", KMeans(n_clusters=K, n_init=10, random_state=42)),\n",
        "        ]\n",
        "    )\n",
        "    pipe.fit(X)\n",
        "    inertia = pipe.named_steps[\"kmeans\"].inertia_\n",
        "    inertias.append(inertia)\n",
        "\n",
        "    # Silhouette on the standardized space\n",
        "    X_scaled = pipe.named_steps[\"scaler\"].transform(X)\n",
        "    labels = pipe.named_steps[\"kmeans\"].labels_\n",
        "    sil = silhouette_score(X_scaled, labels)\n",
        "    sil_scores.append(sil)\n",
        "\n",
        "# Elbow plot\n",
        "plt.figure()\n",
        "plt.plot(list(K_range), inertias, marker=\"o\")\n",
        "plt.xlabel(\"Number of clusters K\")\n",
        "plt.ylabel(\"Inertia (within-cluster sum of squares)\")\n",
        "plt.title(\"Elbow method for K-Means\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"elbow_inertia.png\", dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# Silhouette vs K\n",
        "plt.figure()\n",
        "plt.plot(list(K_range), sil_scores, marker=\"o\")\n",
        "plt.xlabel(\"Number of clusters K\")\n",
        "plt.ylabel(\"Average silhouette score\")\n",
        "plt.title(\"Silhouette score vs K\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"silhouette_score_by_k.png\", dpi=300)\n",
        "plt.close()\n",
        "\n",
        "list(zip(K_range, inertias, sil_scores))"
      ],
      "metadata": {
        "id": "LpdBIaro4OAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the elbow and silhouette diagnostics, many analyses (and the literature) settle on K ≈ 5 for this dataset. We can also pick K that maximizes the silhouette.\n",
        "#\n",
        " We'll:\n",
        " - compute the best K by silhouette,\n",
        " - and you can override it manually if you want a specific K."
      ],
      "metadata": {
        "id": "adTewJtg4PNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_K_sil = K_range[int(np.argmax(sil_scores))]\n",
        "print(f\"Best K by silhouette: {best_K_sil}\")\n",
        "\n",
        "# You can override this if you want a fixed K (e.g., 5).\n",
        "K_final = best_K_sil  # or set to 5 explicitly\n",
        "print(f\"Using K_final = {K_final}\")"
      ],
      "metadata": {
        "id": "nlLBalcy4YBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Fit final K-Means and interpret clusters\n",
        "#\n",
        "We fit a final K-Means model with K_final clusters on standardized income and spending score.\n",
        " Then:\n",
        " - Attach cluster labels to the original DataFrame.\n",
        " - Compute centroids in the ORIGINAL feature units.\n",
        " - Visualize clusters in the income-spending plane."
      ],
      "metadata": {
        "id": "gVclCrWm4asm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans_pipe = Pipeline(\n",
        "    steps=[\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"kmeans\", KMeans(n_clusters=K_final, n_init=10, random_state=42)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "kmeans_pipe.fit(X)\n",
        "X_scaled = kmeans_pipe.named_steps[\"scaler\"].transform(X)\n",
        "labels = kmeans_pipe.named_steps[\"kmeans\"].labels_\n",
        "df[\"cluster\"] = labels\n",
        "\n",
        "# Back-transform centroids to original feature space\n",
        "centers_scaled = kmeans_pipe.named_steps[\"kmeans\"].cluster_centers_\n",
        "centers_orig = kmeans_pipe.named_steps[\"scaler\"].inverse_transform(centers_scaled)\n",
        "centers_df = pd.DataFrame(centers_orig, columns=feature_cols)\n",
        "centers_df[\"cluster\"] = range(K_final)\n",
        "\n",
        "centers_df"
      ],
      "metadata": {
        "id": "wZfoXsBs4gsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scatter plot of clusters in income-spending space with centroids overlaid."
      ],
      "metadata": {
        "id": "EjphkaNH4j6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(\n",
        "    data=df,\n",
        "    x=\"Annual Income (k$)\",\n",
        "    y=\"Spending Score (1-100)\",\n",
        "    hue=\"cluster\",\n",
        "    palette=\"tab10\",\n",
        "    s=60,\n",
        "    alpha=0.8,\n",
        ")\n",
        "plt.scatter(\n",
        "    centers_df[\"Annual Income (k$)\"],\n",
        "    centers_df[\"Spending Score (1-100)\"],\n",
        "    s=200,\n",
        "    c=\"black\",\n",
        "    marker=\"X\",\n",
        "    label=\"Centroids\",\n",
        ")\n",
        "plt.title(f\"K-Means Clusters (K = {K_final})\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"kmeans_clusters_income_spend.png\", dpi=300)\n",
        "plt.close()\n",
        "\n",
        "# Silhouette score for final clustering\n",
        "final_sil = silhouette_score(X_scaled, labels)\n",
        "print(f\"Final average silhouette score (K={K_final}): {final_sil:.3f}\")"
      ],
      "metadata": {
        "id": "s2LgMiqv4m-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Prepare labeled data for discriminant analysis\n",
        "#\n",
        " We now treat the K-Means cluster labels as class labels for supervised classification.\n",
        " We'll:\n",
        " - Create X and y arrays.\n",
        " - Split into train (80%) and test (20%) sets with stratification."
      ],
      "metadata": {
        "id": "OSIr9MkA4pyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_clf = X.values  # same feature set as clustering\n",
        "y_clf = df[\"cluster\"].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_clf, y_clf, test_size=0.2, stratify=y_clf, random_state=42\n",
        ")\n",
        "\n",
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "id": "A2gBKXur4uqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Linear Discriminant Analysis (LDA)\n",
        "#\n",
        " We build an LDA classifier in a pipeline:\n",
        " - StandardScaler\n",
        " - LinearDiscriminantAnalysis\n",
        "#\n",
        " Then we:\n",
        " - Fit on the training set.\n",
        " - Evaluate on the test set with accuracy, confusion matrix, and classification report.\n",
        " - Save confusion matrix as a PNG."
      ],
      "metadata": {
        "id": "YVgAgw134xY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda_pipe = Pipeline(\n",
        "    steps=[\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"lda\", LinearDiscriminantAnalysis()),\n",
        "    ]\n",
        ")\n",
        "\n",
        "lda_pipe.fit(X_train, y_train)\n",
        "y_pred_lda = lda_pipe.predict(X_test)\n",
        "\n",
        "acc_lda = accuracy_score(y_test, y_pred_lda)\n",
        "print(f\"LDA test accuracy: {acc_lda:.3f}\\n\")\n",
        "\n",
        "print(\"LDA classification report:\")\n",
        "print(classification_report(y_test, y_pred_lda))\n",
        "\n",
        "cm_lda = confusion_matrix(y_test, y_pred_lda)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_lda, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"LDA Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"lda_confusion_matrix.png\", dpi=300)\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "wJ0zGTyT43VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Quadratic Discriminant Analysis (QDA)\n",
        "#\n",
        "Same pipeline structure as LDA, but using QuadraticDiscriminantAnalysis.\n",
        "We again evaluate on the test set and save the confusion matrix plot."
      ],
      "metadata": {
        "id": "ntZ2CosW471h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qda_pipe = Pipeline(\n",
        "    steps=[\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"qda\", QuadraticDiscriminantAnalysis()),\n",
        "    ]\n",
        ")\n",
        "\n",
        "qda_pipe.fit(X_train, y_train)\n",
        "y_pred_qda = qda_pipe.predict(X_test)\n",
        "\n",
        "acc_qda = accuracy_score(y_test, y_pred_qda)\n",
        "print(f\"QDA test accuracy: {acc_qda:.3f}\\n\")\n",
        "\n",
        "print(\"QDA classification report:\")\n",
        "print(classification_report(y_test, y_pred_qda))\n",
        "\n",
        "cm_qda = confusion_matrix(y_test, y_pred_qda)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_qda, annot=True, fmt=\"d\", cmap=\"Oranges\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"QDA Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"qda_confusion_matrix.png\", dpi=300)\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "Si7frtzB5AC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Decision boundary visualization for LDA and QDA\n",
        "#\n",
        " We visualize decision boundaries in the 2D feature space:\n",
        " - Annual Income (k$)\n",
        " - Spending Score (1-100)\n",
        "#\n",
        " We use the fitted pipelines to predict class labels on a grid and show\n",
        " the boundaries along with the true K-Means clusters."
      ],
      "metadata": {
        "id": "iwnRV0AA5Dj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ef plot_decision_boundary(model, X, y, feature_names, fname, title):\n",
        "    \"\"\"\n",
        "    Plot decision boundaries for a 2D classifier model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : sklearn Pipeline\n",
        "        Must support .predict() on 2D input.\n",
        "    X : ndarray, shape (n_samples, 2)\n",
        "        Input data in ORIGINAL feature units.\n",
        "    y : ndarray, shape (n_samples,)\n",
        "        Class labels.\n",
        "    feature_names : list of str\n",
        "        Names of the two features [x1_name, x2_name].\n",
        "    fname : str\n",
        "        Filename to save under FIG_DIR.\n",
        "    title : str\n",
        "        Plot title.\n",
        "    \"\"\"\n",
        "    x_min, x_max = X[:, 0].min() - 5, X[:, 0].max() + 5\n",
        "    y_min, y_max = X[:, 1].min() - 5, X[:, 1].max() + 5\n",
        "\n",
        "    xx, yy = np.meshgrid(\n",
        "        np.linspace(x_min, x_max, 300),\n",
        "        np.linspace(y_min, y_max, 300),\n",
        "    )\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "    Z = model.predict(grid)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3, levels=np.arange(Z.max() + 2) - 0.5, cmap=\"tab10\")\n",
        "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"tab10\", edgecolor=\"k\", s=50)\n",
        "    plt.xlabel(feature_names[0])\n",
        "    plt.ylabel(feature_names[1])\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(FIG_DIR / fname, dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "feature_names = feature_cols  # [\"Annual Income (k$)\", \"Spending Score (1-100)\"]\n",
        "\n",
        "# Fit LDA and QDA on the FULL dataset for cleaner boundaries\n",
        "lda_pipe_full = Pipeline(\n",
        "    steps=[(\"scaler\", StandardScaler()), (\"lda\", LinearDiscriminantAnalysis())]\n",
        ")\n",
        "lda_pipe_full.fit(X_clf, y_clf)\n",
        "\n",
        "qda_pipe_full = Pipeline(\n",
        "    steps=[(\"scaler\", StandardScaler()), (\"qda\", QuadraticDiscriminantAnalysis())]\n",
        ")\n",
        "qda_pipe_full.fit(X_clf, y_clf)\n",
        "\n",
        "plot_decision_boundary(\n",
        "    lda_pipe_full,\n",
        "    X_clf,\n",
        "    y_clf,\n",
        "    feature_names,\n",
        "    fname=\"lda_decision_boundary.png\",\n",
        "    title=\"LDA Decision Boundaries and K-Means Clusters\",\n",
        ")\n",
        "\n",
        "plot_decision_boundary(\n",
        "    qda_pipe_full,\n",
        "    X_clf,\n",
        "    y_clf,\n",
        "    feature_names,\n",
        "    fname=\"qda_decision_boundary.png\",\n",
        "    title=\"QDA Decision Boundaries and K-Means Clusters\",\n",
        ")"
      ],
      "metadata": {
        "id": "K-7-pgBe5GM1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}