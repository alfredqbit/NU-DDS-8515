{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtjlh2B/ZDo8rWVbg+KJrN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfredqbit/NU-DDS-8515/blob/main/sepulvedaADDS-8515-3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA and Logistic Regression on the UCI Wine Dataset\n",
        "#\n",
        "# This notebook implements the steps described in the LaTeX report:\n",
        "# - Load and explore the UCI Wine dataset\n",
        "# - Standardize features\n",
        "# - Compute PCA (covariance, eigenvalues, eigenvectors)\n",
        "# - Visualize eigenvalues (scree plot) and PC scores\n",
        "# - Train logistic regression on original features\n",
        "# - Train logistic regression on PCA-transformed features\n",
        "# - Compare performance and simple training times"
      ],
      "metadata": {
        "id": "5gKYQyV8LAyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix\n",
        ")\n",
        "from sklearn.pipeline import Pipeline\n",
        "import time\n",
        "\n",
        "\n",
        "# Make plots a bit larger\n",
        "plt.rcParams[\"figure.figsize\"] = (6, 4)"
      ],
      "metadata": {
        "id": "hm5lsqFbLItu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Dataset Selection, Loading, and EDA"
      ],
      "metadata": {
        "id": "T3FAIYCZLPi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "feature_names = wine.feature_names\n",
        "target_names = wine.target_names\n",
        "\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "df[\"target\"] = y\n",
        "\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"\\nHead:\")\n",
        "display(df.head())\n",
        "\n",
        "print(\"\\nInfo:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nSummary statistics:\")\n",
        "display(df.describe())\n",
        "\n",
        "print(\"\\nClass distribution:\")\n",
        "print(df[\"target\"].value_counts().sort_index(), \" (classes:\", list(target_names), \")\")\n",
        "\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "wKxrakiiLdZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# For this dataset, there are no missing values. We will still standardize\n",
        "# all numerical features before performing PCA and training the classifier.\n",
        "\n",
        "# ## Step 2: Standardization (for PCA exploration)\n"
      ],
      "metadata": {
        "id": "vOdNgks-LjJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler_full = StandardScaler()\n",
        "Z = scaler_full.fit_transform(X)  # standardized features for PCA visualization\n",
        "\n",
        "Z_df = pd.DataFrame(Z, columns=feature_names)\n",
        "display(Z_df.head())"
      ],
      "metadata": {
        "id": "VqgVu_XnLnzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: PCA Computation (Covariance, Eigenvalues, Scree Plot)"
      ],
      "metadata": {
        "id": "bLH4SOYuLsRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Covariance matrix of standardized data\n",
        "S = np.cov(Z.T)\n",
        "print(\"Covariance matrix shape:\", S.shape)\n",
        "\n",
        "# Eigen-decomposition\n",
        "eigvals, eigvecs = np.linalg.eigh(S)  # eigh since S is symmetric\n",
        "# Sort in descending order\n",
        "idx = np.argsort(eigvals)[::-1]\n",
        "eigvals = eigvals[idx]\n",
        "eigvecs = eigvecs[:, idx]\n",
        "\n",
        "print(\"Eigenvalues (descending):\")\n",
        "print(eigvals)\n",
        "\n",
        "explained_var = eigvals / eigvals.sum()\n",
        "cum_explained_var = np.cumsum(explained_var)\n",
        "\n",
        "print(\"\\nProportion of variance explained:\")\n",
        "print(explained_var)\n",
        "print(\"\\nCumulative proportion:\")\n",
        "print(cum_explained_var)\n",
        "\n",
        "# Scree plot from manual eigenvalues\n",
        "plt.figure()\n",
        "plt.plot(range(1, len(eigvals) + 1), eigvals, marker=\"o\")\n",
        "plt.xlabel(\"Principal Component\")\n",
        "plt.ylabel(\"Eigenvalue\")\n",
        "plt.title(\"Scree Plot (Wine Data)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"scree_plot.png\", dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Flfyz6lwLxIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute PCA with scikit-learn to obtain scores and compare explained variance.\n"
      ],
      "metadata": {
        "id": "FAHkRntnL2yC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca_full = PCA()\n",
        "pca_full.fit(Z)\n",
        "\n",
        "print(\"Explained variance ratio (sklearn):\")\n",
        "print(pca_full.explained_variance_ratio_)\n",
        "print(\"Cumulative:\")\n",
        "print(np.cumsum(pca_full.explained_variance_ratio_))\n",
        "\n",
        "# Choose M components to explain >= 95% variance\n",
        "cum_ratio = np.cumsum(pca_full.explained_variance_ratio_)\n",
        "M_95 = np.where(cum_ratio >= 0.95)[0][0] + 1\n",
        "print(f\"\\nNumber of components to reach >= 95% variance: M = {M_95}\")\n"
      ],
      "metadata": {
        "id": "-4l_Oi7QL6p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Visualizing PC Scores (PC1 vs PC2)"
      ],
      "metadata": {
        "id": "K_ymZ15RMBZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = pca_full.transform(Z)  # PCA scores for all samples\n",
        "pc1 = scores[:, 0]\n",
        "pc2 = scores[:, 1]\n",
        "\n",
        "plt.figure()\n",
        "for class_index, class_name in enumerate(target_names):\n",
        "    mask = (y == class_index)\n",
        "    plt.scatter(pc1[mask], pc2[mask], alpha=0.7, label=class_name)\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"PCA Scores: PC1 vs PC2 (Wine Data)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"pc1_pc2_scatter.png\", dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fm4UI0kCMFEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Baseline Model on Original Features\n",
        "#\n",
        "# Train a multiclass logistic regression model on standardized original features."
      ],
      "metadata": {
        "id": "raCJeGPqMJMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "baseline_pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", LogisticRegression(max_iter=1000, multi_class=\"auto\"))\n",
        "])\n",
        "\n",
        "t0 = time.time()\n",
        "baseline_pipeline.fit(X_train, y_train)\n",
        "t1 = time.time()\n",
        "\n",
        "y_pred_baseline = baseline_pipeline.predict(X_test)\n",
        "\n",
        "acc_baseline = accuracy_score(y_test, y_pred_baseline)\n",
        "cm_baseline = confusion_matrix(y_test, y_pred_baseline)\n",
        "\n",
        "print(\"Baseline logistic regression on original features\")\n",
        "print(\"Accuracy:\", acc_baseline)\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(y_test, y_pred_baseline, target_names=target_names))\n",
        "print(\"Confusion matrix:\")\n",
        "print(cm_baseline)\n",
        "print(\"Training time (s):\", t1 - t0)"
      ],
      "metadata": {
        "id": "4OifLQr2MNF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: PCA Transformation (Train and Test)"
      ],
      "metadata": {
        "id": "lcHXvu9GMa6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll use M = 10 components (around 95% variance in this dataset).\n",
        "M = 10\n",
        "\n",
        "pca_pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"pca\", PCA(n_components=M)),\n",
        "    (\"clf\", LogisticRegression(max_iter=1000, multi_class=\"auto\"))\n",
        "])\n",
        "\n",
        "t0 = time.time()\n",
        "pca_pipeline.fit(X_train, y_train)\n",
        "t1 = time.time()\n",
        "\n",
        "y_pred_pca = pca_pipeline.predict(X_test)\n",
        "\n",
        "acc_pca = accuracy_score(y_test, y_pred_pca)\n",
        "cm_pca = confusion_matrix(y_test, y_pred_pca)\n",
        "\n",
        "print(f\"PCA + logistic regression (M = {M} PCs)\")\n",
        "print(\"Accuracy:\", acc_pca)\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(y_test, y_pred_pca, target_names=target_names))\n",
        "print(\"Confusion matrix:\")\n",
        "print(cm_pca)\n",
        "print(\"Training time (s):\", t1 - t0)"
      ],
      "metadata": {
        "id": "5ZQNb5YiMb-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For comparison, we can also examine a very aggressive reduction to only 2 PCs.\n"
      ],
      "metadata": {
        "id": "aB4nFZ19MiiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca2_pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"pca\", PCA(n_components=2)),\n",
        "    (\"clf\", LogisticRegression(max_iter=1000, multi_class=\"auto\"))\n",
        "])\n",
        "\n",
        "t0 = time.time()\n",
        "pca2_pipeline.fit(X_train, y_train)\n",
        "t1 = time.time()\n",
        "\n",
        "y_pred_pca2 = pca2_pipeline.predict(X_test)\n",
        "\n",
        "acc_pca2 = accuracy_score(y_test, y_pred_pca2)\n",
        "cm_pca2 = confusion_matrix(y_test, y_pred_pca2)\n",
        "\n",
        "print(\"PCA + logistic regression (M = 2 PCs)\")\n",
        "print(\"Accuracy:\", acc_pca2)\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(y_test, y_pred_pca2, target_names=target_names))\n",
        "print(\"Confusion matrix:\")\n",
        "print(cm_pca2)\n",
        "print(\"Training time (s):\", t1 - t0)"
      ],
      "metadata": {
        "id": "i0lAUhwRMorN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Compare Model Performance"
      ],
      "metadata": {
        "id": "r5mH0luCMtvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame({\n",
        "    \"Model\": [\"Baseline (13 features)\", \"PCA (10 PCs)\", \"PCA (2 PCs)\"],\n",
        "    \"Accuracy\": [acc_baseline, acc_pca, acc_pca2],\n",
        "})\n",
        "\n",
        "display(results)\n",
        "\n",
        "plt.figure()\n",
        "plt.bar(results[\"Model\"], results[\"Accuracy\"])\n",
        "plt.ylim(0.8, 1.0)\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy Comparison: Original vs PCA-Transformed Features\")\n",
        "plt.xticks(rotation=20, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"accuracy_comparison.png\", dpi=300)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zp8F_6fYMy25"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}