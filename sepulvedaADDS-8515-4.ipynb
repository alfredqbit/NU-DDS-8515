{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdOEe4vqkwBsax953Cc374",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfredqbit/NU-DDS-8515/blob/main/sepulvedaADDS-8515-4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Factor Analysis and Logistic Regression on the Consumer Behavior Dataset\n",
        "\n",
        "This notebook applies Factor Analysis (FA) to the Consumer Behavior dataset. We perform preprocessing,\n",
        "factor extraction and rotation, and evaluate how FA impacts machine learning model performance."
      ],
      "metadata": {
        "id": "gp99SKXboP8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.decomposition import FactorAnalysis\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create figures directory\n",
        "FIG_DIR = \"figures\"\n",
        "os.makedirs(FIG_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "N-oGYry9obnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Dataset Selection, Loading, and Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "d635TH42ojCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create synthetic data for illustration purposes\n",
        "np.random.seed(42)\n",
        "n_samples = 200\n",
        "n_features = 10\n",
        "X = np.random.randn(n_samples, n_features)\n",
        "y = np.random.randint(0, 2, n_samples)  # Binary classification\n",
        "\n",
        "# Create DataFrame\n",
        "feature_names = [f'Feature_{i+1}' for i in range(n_features)]\n",
        "df = pd.DataFrame(X, columns=feature_names)\n",
        "df['target'] = y\n",
        "\n",
        "# Show basic information\n",
        "df.head(), df.describe(), df.isnull().sum(), df['target'].value_counts()"
      ],
      "metadata": {
        "id": "e7BhpiPgonNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Data Preprocessing and Feature Engineering"
      ],
      "metadata": {
        "id": "9be240elovns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(df.drop(columns='target'))\n",
        "\n",
        "# Check standardized data\n",
        "pd.DataFrame(X_scaled, columns=feature_names).head()"
      ],
      "metadata": {
        "id": "ehc95I_fo1Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Conduct Factor Analysis"
      ],
      "metadata": {
        "id": "YaLT_A-bo_gS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check correlation matrix\n",
        "correlation_matrix = np.corrcoef(X_scaled.T)\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm')\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.savefig(os.path.join(FIG_DIR, 'correlation_matrix.png'))\n",
        "plt.show()\n",
        "\n",
        "# Perform Factor Analysis\n",
        "fa = FactorAnalysis(n_components=3)\n",
        "fa.fit(X_scaled)\n",
        "\n",
        "# Factor loadings\n",
        "loadings_df = pd.DataFrame(fa.components_, columns=feature_names)\n",
        "loadings_df"
      ],
      "metadata": {
        "id": "I207yDiCqtGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Data Visualization and Interpretation"
      ],
      "metadata": {
        "id": "tXYM5dKmpSIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the factor loadings\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(loadings_df, annot=True, cmap='coolwarm')\n",
        "plt.title('Factor Loadings for FA Components')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIG_DIR, 'factor_loadings.png'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1beUC6PzpXU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Machine Learning Model on Original Features (LR)"
      ],
      "metadata": {
        "id": "W8dxzDfEpla5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression pipeline on original features\n",
        "baseline_pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", LogisticRegression(max_iter=1000, multi_class=\"auto\"))\n",
        "])\n",
        "\n",
        "t0 = time.time()\n",
        "baseline_pipeline.fit(X_train, y_train)\n",
        "t1 = time.time()\n",
        "\n",
        "y_pred_baseline = baseline_pipeline.predict(X_test)\n",
        "\n",
        "print(f\"Baseline model accuracy: {accuracy_score(y_test, y_pred_baseline)}\")\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(y_test, y_pred_baseline))\n",
        "print(f\"Training time (s): {t1 - t0}\")"
      ],
      "metadata": {
        "id": "Jhb4lVVXpsZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Apply Factor Analysis to Transform Data"
      ],
      "metadata": {
        "id": "aSgyiGN3ptwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_fa = fa.transform(X_scaled)"
      ],
      "metadata": {
        "id": "bRHiQUbNp5-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Train Model on FA-transformed Data"
      ],
      "metadata": {
        "id": "i5Nktf85p_2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression pipeline on FA-transformed features\n",
        "pca_pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"fa\", FactorAnalysis(n_components=3)),  # Same number of components\n",
        "    (\"clf\", LogisticRegression(max_iter=1000, multi_class=\"auto\"))\n",
        "])\n",
        "\n",
        "t0 = time.time()\n",
        "pca_pipeline.fit(X_train, y_train)\n",
        "t1 = time.time()\n",
        "\n",
        "y_pred_fa = pca_pipeline.predict(X_test)\n",
        "\n",
        "print(f\"FA-transformed model accuracy: {accuracy_score(y_test, y_pred_fa)}\")\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(y_test, y_pred_fa))\n",
        "print(f\"Training time (s): {t1 - t0}\")"
      ],
      "metadata": {
        "id": "PwAXSg-aqGpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8: Compare Performance Before and After FA"
      ],
      "metadata": {
        "id": "oJQpc0w4qK8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy comparison\n",
        "results = pd.DataFrame({\n",
        "    \"Model\": [\"Baseline (Original Features)\", \"FA-transformed Features\"],\n",
        "    \"Accuracy\": [accuracy_score(y_test, y_pred_baseline), accuracy_score(y_test, y_pred_fa)],\n",
        "})\n",
        "\n",
        "# Save the results as PNG and CSV\n",
        "results.to_csv(os.path.join(FIG_DIR, 'results_table.csv'), index=False)\n",
        "\n",
        "# Bar chart comparison\n",
        "plt.figure()\n",
        "plt.bar(results[\"Model\"], results[\"Accuracy\"])\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Model Accuracy Comparison: Original vs FA-Transformed Features\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIG_DIR, 'accuracy_comparison.png'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GXD321xAqUiQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}