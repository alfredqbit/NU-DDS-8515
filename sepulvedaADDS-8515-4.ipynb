{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V6E1",
      "authorship_tag": "ABX9TyNHuxXISiHgkNAkvaXsFhw0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfredqbit/NU-DDS-8515/blob/main/sepulvedaADDS-8515-4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 4: Implementing Factor Analysis and Evaluating Machine Learning Model Performance\n",
        "\n",
        "Factor Analysis and Logistic Regression on the Consumer Behavior Dataset\n",
        "\n",
        "This notebook applies Factor Analysis (FA) to the Consumer Behavior dataset. We perform preprocessing,\n",
        "factor extraction and rotation, and evaluate how FA impacts machine learning model performance."
      ],
      "metadata": {
        "id": "gp99SKXboP8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.decomposition import FactorAnalysis\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create figures directory\n",
        "FIG_DIR = \"figures\"\n",
        "os.makedirs(FIG_DIR, exist_ok=True)\n",
        "\n",
        "# Helper function to save DataFrame as PNG\n",
        "def save_df_as_png(df_to_save, filename, title=\"\"):\n",
        "    fig, ax = plt.subplots(figsize=(12, len(df_to_save.index) * 0.4 + 1)) # Adjust figsize dynamically\n",
        "    ax.axis('off')\n",
        "    table = ax.table(cellText=df_to_save.values, colLabels=df_to_save.columns, loc='center')\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(10)\n",
        "    table.scale(1.2, 1.2)\n",
        "    ax.set_title(title, fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.close() # Close the figure to free memory\n",
        "\n",
        "# Helper function to save text as PNG\n",
        "def save_text_as_png(text_content, filename, title=\"\"):\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    ax.axis('off')\n",
        "    ax.text(0.01, 0.99, text_content, va='top', ha='left', fontsize=10, fontfamily='monospace', transform=ax.transAxes)\n",
        "    ax.set_title(title, fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(filename)\n",
        "    plt.close() # Close the figure to free memory\n"
      ],
      "metadata": {
        "id": "N-oGYry9obnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Dataset Selection, Loading, and Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "d635TH42ojCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the UCI Online Shoppers Purchasing Intention Dataset\n",
        "# Dataset URL from UCI Machine Learning Repository\n",
        "data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00468/online_shoppers_intention.csv\"\n",
        "df = pd.read_csv(data_url)\n",
        "\n",
        "# The target variable is 'Revenue', which is boolean. Convert it to integer (0 or 1).\n",
        "df['Revenue'] = df['Revenue'].astype(int)\n",
        "\n",
        "# Identify features and target.\n",
        "# Drop categorical columns for now to align with numerical nature of Factor Analysis\n",
        "# and the original synthetic data which was purely numerical. These can be one-hot encoded later if needed.\n",
        "categorical_cols = ['Month', 'OperatingSystems', 'Browser', 'Region', 'TrafficType', 'VisitorType', 'Weekend']\n",
        "numerical_features_df = df.drop(columns=categorical_cols + ['Revenue'])\n",
        "\n",
        "y = df['Revenue']\n",
        "X = numerical_features_df\n",
        "\n",
        "# Update feature_names based on the numerical columns remaining in X\n",
        "feature_names = X.columns.tolist()\n",
        "\n",
        "# Show basic information for the loaded dataset\n",
        "# Create a directory for exported tables\n",
        "TABLE_DIR = \"tables\"\n",
        "os.makedirs(TABLE_DIR, exist_ok=True)\n",
        "\n",
        "# Export df.head() as PNG\n",
        "save_df_as_png(df.head(), os.path.join(FIG_DIR, 'df_head.png'), title='DataFrame Head')\n",
        "print(f\"Exported df_head.png to {FIG_DIR}\")\n",
        "\n",
        "# Export df.describe() as CSV and PNG\n",
        "save_df_as_png(df.describe(), os.path.join(FIG_DIR, 'df_describe.png'), title='DataFrame Description')\n",
        "print(f\"Exported df_describe.png to {FIG_DIR}\")\n",
        "\n",
        "# Export df.isnull().sum() as CSV and PNG\n",
        "save_df_as_png(df.isnull().sum().to_frame(name='Missing Values'), os.path.join(FIG_DIR, 'df_isnull_sum.png'), title='Missing Values per Column')\n",
        "print(f\"Exported df_isnull_sum.png to {FIG_DIR}\")\n",
        "\n",
        "print(\"DataFrame head:\")\n",
        "display(df.head())\n",
        "print(\"\\nDataFrame Description:\")\n",
        "display(df.describe())\n",
        "print(\"\\nMissing values per column:\")\n",
        "display(df.isnull().sum())\n",
        "print(\"\\nTarget variable value counts:\")\n",
        "display(df['Revenue'].value_counts())\n"
      ],
      "metadata": {
        "id": "e7BhpiPgonNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Data Preprocessing and Feature Engineering\n",
        "- standardize data"
      ],
      "metadata": {
        "id": "9be240elovns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X) # Use the 'X' DataFrame prepared in the previous step\n",
        "\n",
        "# Check standardized data\n",
        "X_scaled_df_head = pd.DataFrame(X_scaled, columns=feature_names).head()\n",
        "display(X_scaled_df_head)\n",
        "\n",
        "# Export df.head() after standardization as PNG\n",
        "save_df_as_png(X_scaled_df_head, os.path.join(FIG_DIR, 'df_scaled_head.png'), title='Standardized DataFrame Head')\n",
        "print(f\"Exported df_scaled_head.png to {FIG_DIR}\")\n"
      ],
      "metadata": {
        "id": "ehc95I_fo1Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Conduct Factor Analysis\n",
        "\n",
        "- plot correlation heatmap\n",
        "- perform factor analysis with 3 components\n",
        "- form factor loading\n",
        "\n"
      ],
      "metadata": {
        "id": "YaLT_A-bo_gS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame from the scaled data to preserve feature names for the heatmap\n",
        "X_scaled_df = pd.DataFrame(X_scaled, columns=feature_names)\n",
        "\n",
        "# Check correlation matrix\n",
        "correlation_matrix = X_scaled_df.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', xticklabels=feature_names, yticklabels=feature_names)\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.savefig(os.path.join(FIG_DIR, 'correlation_matrix.png'))\n",
        "plt.show()\n",
        "\n",
        "# Perform Factor Analysis\n",
        "fa = FactorAnalysis(n_components=3)\n",
        "fa.fit(X_scaled)\n",
        "\n",
        "# Factor loadings\n",
        "loadings_df = pd.DataFrame(fa.components_, columns=feature_names)\n",
        "loadings_df"
      ],
      "metadata": {
        "id": "I207yDiCqtGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Data Visualization and Interpretation\n",
        "- display and store heatmap of loading dataframe"
      ],
      "metadata": {
        "id": "tXYM5dKmpSIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the factor loadings\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(loadings_df, annot=True, cmap='coolwarm')\n",
        "plt.title('Factor Loadings for FA Components')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIG_DIR, 'factor_loadings.png'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1beUC6PzpXU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Machine Learning Model on Original Features (LR)\n",
        "- form Python pieline: standardization and logistic regression modeling\n",
        "- display accuracy metrics and training time"
      ],
      "metadata": {
        "id": "W8dxzDfEpla5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Logistic Regression pipeline on original features\n",
        "baseline_pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
        "])\n",
        "\n",
        "t0 = time.time()\n",
        "baseline_pipeline.fit(X_train, y_train)\n",
        "t1 = time.time()\n",
        "\n",
        "y_pred_baseline = baseline_pipeline.predict(X_test)\n",
        "\n",
        "print(f\"Baseline model accuracy: {accuracy_score(y_test, y_pred_baseline)}\")\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(y_test, y_pred_baseline))\n",
        "print(f\"Training time (s): {t1 - t0}\")"
      ],
      "metadata": {
        "id": "Jhb4lVVXpsZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Apply Factor Analysis to Transform Data"
      ],
      "metadata": {
        "id": "aSgyiGN3ptwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_fa = fa.transform(X_scaled)"
      ],
      "metadata": {
        "id": "bRHiQUbNp5-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: Train Model on FA-transformed Data\n",
        "- execute a standardization-factor analysis-logisitc regression pipeline"
      ],
      "metadata": {
        "id": "i5Nktf85p_2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression pipeline on FA-transformed features\n",
        "pca_pipeline = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"fa\", FactorAnalysis(n_components=3)),  # Same number of components\n",
        "    (\"clf\", LogisticRegression(max_iter=1000, multi_class=\"auto\", class_weight='balanced'))\n",
        "])\n",
        "\n",
        "t0 = time.time()\n",
        "pca_pipeline.fit(X_train, y_train)\n",
        "t1 = time.time()\n",
        "\n",
        "y_pred_fa = pca_pipeline.predict(X_test)\n",
        "\n",
        "print(f\"FA-transformed model accuracy: {accuracy_score(y_test, y_pred_fa)}\")\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(y_test, y_pred_fa))\n",
        "print(f\"Training time (s): {t1 - t0}\")\n",
        "\n",
        "# Export classification report for baseline model as TXT and PNG\n",
        "baseline_report = classification_report(y_test, y_pred_baseline)\n",
        "save_text_as_png(baseline_report, os.path.join(FIG_DIR, 'classification_report_baseline.png'), title='Classification Report: Baseline Model')\n",
        "print(f\"Exported classification_report_baseline.png to {FIG_DIR}\")\n",
        "\n",
        "# Export classification report for FA-transformed model as TXT and PNG\n",
        "fa_report = classification_report(y_test, y_pred_fa)\n",
        "save_text_as_png(fa_report, os.path.join(FIG_DIR, 'classification_report_fa.png'), title='Classification Report: FA-transformed Model')\n",
        "print(f\"Exported classification_report_fa.png to {FIG_DIR}\")\n"
      ],
      "metadata": {
        "id": "PwAXSg-aqGpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abc566fa"
      },
      "source": [
        "Confusion Matrix for Baseline Model (Original Features)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ee4231e"
      },
      "source": [
        "cm_baseline = confusion_matrix(y_test, y_pred_baseline)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_baseline, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title('Confusion Matrix: Baseline Model')\n",
        "plt.ylabel('Actual Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.savefig(os.path.join(FIG_DIR, 'confusion_matrix_baseline.png'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "933f350d"
      },
      "source": [
        "Confusion Matrix for FA-transformed Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10c39807"
      },
      "source": [
        "cm_fa = confusion_matrix(y_test, y_pred_fa)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_fa, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title('Confusion Matrix: FA-transformed Model')\n",
        "plt.ylabel('Actual Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.savefig(os.path.join(FIG_DIR, 'confusion_matrix_fa.png'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8: Compare Performance Before and After FA\n",
        "- store accuracy metrics before and after FA\n",
        "- show and store bar chart of accuracy comparison before and after FA"
      ],
      "metadata": {
        "id": "oJQpc0w4qK8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy comparison\n",
        "results = pd.DataFrame({\n",
        "    \"Model\": [\"Baseline (Original Features)\", \"FA-transformed Features\"],\n",
        "    \"Accuracy\": [accuracy_score(y_test, y_pred_baseline), accuracy_score(y_test, y_pred_fa)],\n",
        "})\n",
        "\n",
        "# Save the results as PNG\n",
        "save_df_as_png(results, os.path.join(FIG_DIR, 'results_table.png'), title='Model Accuracy Comparison Table')\n",
        "print(f\"Exported results_table.png to {FIG_DIR}\")\n",
        "\n",
        "# Bar chart comparison\n",
        "plt.figure()\n",
        "plt.bar(results[\"Model\"], results[\"Accuracy\"])\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Model Accuracy Comparison: Original vs FA-Transformed Features\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(FIG_DIR, 'accuracy_comparison.png'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GXD321xAqUiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40348ff4"
      },
      "source": [
        "### Scree Plot\n",
        "A Scree Plot displays the eigenvalues associated with a component or factor in descending order versus the number of the component or factor. It helps in determining the number of factors to retain."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e47b9b4"
      },
      "source": [
        "# Calculate eigenvalues of the correlation matrix for the Scree Plot\n",
        "corr_matrix = np.corrcoef(X_scaled.T)\n",
        "eigenvalues = np.linalg.eigvalsh(corr_matrix)\n",
        "eigenvalues = eigenvalues[::-1] # Sort in descending order\n",
        "\n",
        "# Plot Scree Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(eigenvalues) + 1), eigenvalues, marker='o', linestyle='-')\n",
        "plt.title('Scree Plot')\n",
        "plt.xlabel('Number of Factors')\n",
        "plt.ylabel('Eigenvalue')\n",
        "plt.axhline(y=1, color='r', linestyle='--', label='Eigenvalue = 1') # Kaiser criterion line\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(FIG_DIR, 'scree_plot.png'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}